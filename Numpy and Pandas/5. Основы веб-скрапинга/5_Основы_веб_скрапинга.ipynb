{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWB4yHPk4JO2"
      },
      "source": [
        "# Обязательная часть\n",
        "Вам необходимо написать функцию, которая будет основана на поиске по сайту habr.com. Функция в качестве параметра должна принимать список запросов для поиска (например, ['python', 'анализ данных']) и на основе материалов, попавших в результаты поиска по каждому запросу, возвращать датафрейм вида:\n",
        "\n",
        "<дата> - <заголовок> - <ссылка на материал>\n",
        "В рамках задания предполагается работа только с одной (первой) страницей результатов поисковой выдачи для каждого запроса. Материалы в датафрейме не должны дублироваться, если они попадали в результаты поиска для нескольких запросов из списка."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DElnl-A64JO8"
      },
      "outputs": [],
      "source": [
        "import requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0VZ64G9m4JO_"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kh0u_9-v4JO_"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PwZrnXaK4JPA"
      },
      "outputs": [],
      "source": [
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VO6h82WU4JPA"
      },
      "outputs": [],
      "source": [
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dq16Hrpt4JPB"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e8XAn1KO4JPB"
      },
      "outputs": [],
      "source": [
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q5wrGcLH4JPC"
      },
      "outputs": [],
      "source": [
        "keywords = ['python', 'парсинг']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-dqCujui4JPD"
      },
      "outputs": [],
      "source": [
        "def get_all_info(url, query):\n",
        "\n",
        "    '''общий результат'''\n",
        "    table = {}\n",
        "\n",
        "    '''достаем ссылки'''\n",
        "    urls = []\n",
        "    params = {\n",
        "        'search_query': 'python' or 'парсинг'\n",
        "    }\n",
        "    res = requests.get(url, params)\n",
        "    soup = BeautifulSoup(res.text, 'html.parser')\n",
        "    page1 = soup.find_all(class_='tm-pagination__page')\n",
        "    lastpage = str(page1[-1])\n",
        "    str1 = re.findall(r'[0-9]', lastpage)\n",
        "\n",
        "    '''узнаем сколько страниц в поиске с помощью костыля'''\n",
        "    pages = 0\n",
        "    if len(str1) == 2:\n",
        "        pages += int(str1[0])\n",
        "    elif len(str1) == 4:\n",
        "        pages += int(str1[0] + str1[1])\n",
        "    elif len(str1) == 6:\n",
        "        pages += int(str1[0] + str1[1] + str1[2])\n",
        "    else:\n",
        "        print ('Too many lists')\n",
        "\n",
        "    '''достаем все имеющиеся ссылки'''\n",
        "    for i in range(1, pages):\n",
        "        params['page'] = i\n",
        "        time.sleep(0.3)\n",
        "        news_blocks = soup.find_all(class_='tm-articles-list__item')\n",
        "        articles_intro = list(map(lambda x: x.find('div', class_='tm-article-body'), news_blocks))\n",
        "        a_list = list(map(lambda x: x.find('a').get('href'), articles_intro))\n",
        "        for link in a_list:\n",
        "            if 'https' not in link:\n",
        "                link = 'https://habr.com' + link\n",
        "                urls.append(link)\n",
        "            else:\n",
        "                urls.append(link)\n",
        "\n",
        "    '''Достаем дату, описание и текст'''\n",
        "    date = []\n",
        "    title = []\n",
        "    text = []\n",
        "\n",
        "    news_blocks = soup.find_all(class_='tm-articles-list__item')\n",
        "    for item in news_blocks:\n",
        "        try:\n",
        "            date.append(item.find_all('span', class_='tm-article-snippet__datetime-published').get_text())\n",
        "            title.append(item.find_all('h2', class_='tm-article-snippet__title tm-article-snippet__title_h2').get_text())\n",
        "            text.append(item.find_all('div', class_='article-formatted-body article-formatted-body_version-2').get_text())\n",
        "\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "\n",
        "    table['date'] = date\n",
        "    table['title'] = title\n",
        "    table['url'] = urls\n",
        "    table_df = pd.DataFrame(table)\n",
        "\n",
        "    return table_df\n",
        "\n",
        "\n",
        "\n",
        "all_ = get_all_info('https://habr.com/ru/all/', 'python' or 'парсинг')\n",
        "alli = all_[0: 10]\n",
        "alli"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THCBPzNK4JPE"
      },
      "source": [
        "# Дополнительная часть (необязательная)\n",
        "Функция из обязательной части задания должна быть расширена следующим образом:\n",
        "\n",
        "кроме списка ключевых слов для поиска необходимо объявить параметр с количеством страниц поисковой выдачи. Т.е. при передаче в функцию аргумента 4 необходимо получить материалы с первых 4 страниц результатов;\n",
        "в датафрейме должны быть столбцы с полным текстом найденных материалов и количеством лайков:\n",
        "<дата> - <заголовок> - <ссылка на материал> - <текст материала> - <количество лайков>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TFDtGdrL4JPE"
      },
      "outputs": [],
      "source": [
        "mails = ['xxx@x.ru', 'yyy@y.com']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hUWkT-G74JPE"
      },
      "outputs": [],
      "source": [
        "request = 'https://identityprotection.avast.com/v1/web/query/site-breaches/unauthorized-data'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j1rbGc_V4JPF"
      },
      "outputs": [],
      "source": [
        "params = {\n",
        "    'Vaar-Version': '0',\n",
        "    'Vaar-Header-App-Product': 'hackcheck-web-avast',\n",
        "    'Vaar-Header-App-Product-Name': 'hackcheck-web-avast',\n",
        "    'Vaar-Header-App-Build-Version': '1.0.0'\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i1rZLWv_4JPF"
      },
      "outputs": [],
      "source": [
        "table = pd.DataFrame(columns = {'email','date','site','title'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VU1Pw73e4JPF"
      },
      "outputs": [],
      "source": [
        "for email in mails:\n",
        "    res = requests.post(request, json={'emailAddresses': [email]},headers = params)\n",
        "    info = json.loads(res.text)\n",
        "    if info:\n",
        "        for key in info['breaches'].keys():\n",
        "            title = extracted_data['breaches'][key]['description']\n",
        "            date = extracted_data['breaches'][key]['publishDate']\n",
        "            source = extracted_data['breaches'][key]['site']\n",
        "            site = extracted_data['breaches'][key]['site']\n",
        "            data = {'email':email,'date':date,'source':source,'title':title}\n",
        "            table = table.append(data,ignore_index=True)\n",
        "table"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}